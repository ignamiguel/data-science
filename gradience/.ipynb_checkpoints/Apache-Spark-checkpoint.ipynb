{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------------------------------------------------- ##\n",
    "## Usar esta configuración                                        ##\n",
    "## -------------------------------------------------------------- ##\n",
    "import pyspark\n",
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3.6\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python3.6\"\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"sample_app\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integersListRDD = sc.parallelize([1,2,3,4,5,6,7,8,9,10], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(integersListRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "path_to_datos_navent = '/Users/ignacio.iglesias/Dev/datos/finger-data/datos_navent_fiuba/'\n",
    "\n",
    "df_vistas = sqlContext.read.csv(path_to_datos_navent + 'fiuba_3_vistas.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vistas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Dado el siguiente código de pyspark e ignorando la definicion de pairRDD \n",
    "\n",
    "`print pairRDD.groupByKey().mapValues(lambda x: sum(x)).collect() print pairRDD.reduceByKey(add).collect()` \n",
    "\n",
    "Obtenemos la siguiente salida\n",
    "\n",
    "`[('a', 3), ('b', 1)] [('a', 3), ('b', 1)]`\n",
    "\n",
    "¿Como debería estar definida pairRDD para obtener ese resultado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pairRDD =  [('a', 1), ('a', 1), ('a', 1), ('b', 1)]\n",
    "#pairRDD = sc.parallelize([('a', 1), ('a', 1), ('a', 1), ('b', 0.5)])\n",
    "#pairRDD = sc.parallelize([('b', 1), ('b', 1), ('b', 1), ('a', 2)])\n",
    "pairRDD = sc.parallelize([('a', 1), ('a', 2), ('b', 1)])\n",
    "print(pairRDD.groupByKey().mapValues(lambda x: sum(x)).collect())\n",
    "print(pairRDD.reduceByKey(add).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pepe(x, y):\n",
    "    return x + y\n",
    "\n",
    "pairRDD.reduceByKey(pepe).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Tengo un RDD con los siguientes campos: \n",
    "\n",
    "`padron, apellido_nombre, fecha_nacimiento, fecha_ingreso, codigo_carrera.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sqlContext.read.csv('test.csv', header=True)\n",
    "type(rdd)\n",
    "rdd = rdd.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12, 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rdd.filter(rdd['codigo_carrera'] == 10)\\\n",
    "#   .filter(rdd['fecha_ingreso'] > '2016-01-01')\\\n",
    "#    .map(lambda row: (len(row.apellido_nombre), 1))\\\n",
    "#     .reduceByKey(lambda a, b: a+b).collect()\n",
    "         \n",
    "rdd.filter(lambda row: row.codigo_carrera == '10')\\\n",
    "   .filter(lambda row: row.fecha_ingreso > '2016-01-01')\\\n",
    "    .map(lambda row: (len(row.apellido_nombre), 1))\\\n",
    "    .reduceByKey(lambda a, b: a+b).collect()            \n",
    "            \n",
    "# obtiene los apellidos de los alumnos que hayan ingresado despues del 2016-01-01 a la carrera 10\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "rdd.filter(lambda row: row.fecha_nacimiento > '1970-01-01')\\\n",
    "   .filter(lambda row: row.fecha_ingreso > '2010-01-01')\\\n",
    "    .map(lambda row: (date.today().year - int(row.fecha_nacimiento[:4]), 1))\\\n",
    "    .reduce(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "    \n",
    "#  obtiene la cantidad de alumnos con fecha de nacimiento anterior al 1970 \n",
    "# y que hayan ingresado despues del 1 de Enero de 2010.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda row: row.fecha_nacimiento > '1970-01-01')\\\n",
    ".filter(lambda row: row.fecha_ingreso > '2010-01-01')\\\n",
    ".map(lambda row: (date.today().year - int(row.fecha_nacimiento[:4]), 1))\\\n",
    ".reduce(lambda a, b: (a[0] + b[0], a[1] + b[1])) \n",
    "\n",
    "# obtiene la edad de los alumnos con fecha de nacimiento anterior al 1970\n",
    "# y que hayan ingresado despues del 1 de Enero de 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2017', 2)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda row: (row.fecha_ingreso[:4], 1))\\\n",
    ".reduceByKey(lambda a, b: a+b).collect() \n",
    "\n",
    "# obtiene la cantidad de inscriptos por año."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(padron='1111', apellido_nombre='GARCIA Pablo', fecha_nacimiento='2017-01-01', fecha_ingreso='2017-01-01', codigo_carrera='10'),\n",
       " Row(padron='2222', apellido_nombre='PEREZ Gaston', fecha_nacimiento='2017-01-01', fecha_ingreso='2017-01-01', codigo_carrera='20')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.filter(lambda row: row.fecha_nacimiento < '1970-01-01')\\\n",
    "   .filter(lambda row: row.fecha_ingreso > '2010-01-01')\\\n",
    "    .map(lambda row: (date.today().year - row.fecha_nacimiento.year, 1))\\\n",
    "    .reduce(lambda a, b: (a[0] + b[0], a[1] + b[1])) \n",
    "    \n",
    "# obtiene la cantidad de alumnos con fecha de nacimiento anterior al 1970\n",
    "# y que hayan ingresado despues del 1 de Enero de 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'2017-01-01' < '1970-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "def lowerThan1970(row):\n",
    "    return row.fecha_nacimiento < '1970-01-01'\n",
    "\n",
    "def graterThan2010(row):\n",
    "    return row.fecha_ingreso > '2010-01-01'\n",
    "\n",
    "def yearDiff(row):\n",
    "    return (date.today().year - row.fecha_nacimiento.year, 1)\n",
    "\n",
    "def doSum(a, b):\n",
    "    return (a[0] + b[0], a[1] + b[1])\n",
    "\n",
    "rdd.filter(lowerThan1970).collect()#\\\n",
    "#.filter(graterThan2010)\\\n",
    "#.map(yearDiff)\\\n",
    "#.reduce(doSum) \n",
    "\n",
    "# obtiene la edad de los alumnos con fecha de nacimiento anterior al 1970 \n",
    "# y que hayan ingresado despues del 1 de Enero de 2010."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Archivo.txt:\n",
    "\n",
    "`datos spark guia\n",
    "spark hadoop \n",
    "guia datos spark gradience`\n",
    "\n",
    "Corremos:\n",
    "\n",
    "`sc.textFile(\"Archivo.txt\")\n",
    ".flatMap(lambda a: a.split())\n",
    ".map(lambda a: (a, 1))\n",
    ".reduceByKey(lambda a, b: a+b)\n",
    ".takeOrdered(3, key = lambda x: -x[1])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark', 3), ('datos', 2), ('guia', 2)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile(\"Archivo.txt\")\\\n",
    ".flatMap(lambda a: a.split())\\\n",
    ".map(lambda a: (a, 1))\\\n",
    ".reduceByKey(lambda a, b: a + b)\\\n",
    ".takeOrdered(3, key = lambda x: -x[1])\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
